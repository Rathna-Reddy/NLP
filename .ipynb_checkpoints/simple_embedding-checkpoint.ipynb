{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9079bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418c74e",
   "metadata": {},
   "source": [
    "Word Embedding\n",
    "There are two techniques for creating word embeddings\n",
    "1. supervised learning\n",
    "2. self supervised learning techniques such as word2vec, glove\n",
    "In this notebook we will do first technique of supervised learning using simple food review classification and see how word embeddings are calculated while solving that problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df10ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create a simple embedding for food reviews\n",
    "#Why Embedding ?\n",
    "#what are the advantages of Embedding ?\n",
    "reviews = ['nice food',\n",
    "          'amazing restaurant',\n",
    "          'too good',\n",
    "          'just loved it',\n",
    "          'will go again',\n",
    "          'horrible food',\n",
    "          'never go there',\n",
    "          'poor service',\n",
    "          'poor quality',\n",
    "          'needs improvement']\n",
    "sentiment = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d7884d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[4, 1],\n",
       " [5, 6],\n",
       " [7, 8],\n",
       " [9, 10, 11],\n",
       " [12, 2, 13],\n",
       " [14, 1],\n",
       " [15, 2, 16],\n",
       " [3, 17],\n",
       " [3, 18],\n",
       " [19]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step-1: convert words into one-hot encoding\n",
    "\n",
    "#for that we need the size of the vocabulary\n",
    "all_words = []\n",
    "for review in reviews:\n",
    "    all_words.extend(review.split())\n",
    "    \n",
    "#extract unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "#get the length of the unique words\n",
    "vocab_size = len(unique_words)\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "# encoded_reviews = [one_hot(review, vocab_size) for review in reviews]\n",
    "# encoded_reviews\n",
    "\n",
    "#create the Tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "\n",
    "#get the encoded sequences\n",
    "encoded_reviews = tokenizer.texts_to_sequences(reviews)\n",
    "encoded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b14c651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  1,  0],\n",
       "       [ 5,  6,  0],\n",
       "       [ 7,  8,  0],\n",
       "       [ 9, 10, 11],\n",
       "       [12,  2, 13],\n",
       "       [14,  1,  0],\n",
       "       [15,  2, 16],\n",
       "       [ 3, 17,  0],\n",
       "       [ 3, 18,  0],\n",
       "       [19,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 3\n",
    "padded_reviews = pad_sequences(encoded_reviews, maxlen=max_length, padding='post')\n",
    "padded_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "656d51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_vector_size = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedded_vector_size, input_length = max_length, name=\"embedding\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a97ff9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_reviews\n",
    "y = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0757ee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 5)              100       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 15)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116\n",
      "Trainable params: 116\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8eeafd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff3131bb6d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f520c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step - loss: 0.6218 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#evaluate the accuracy\n",
    "loss, accuracy = model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8985ead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[[ 0.0716628  -0.09954268 -0.03390084  0.09419224  0.05571979]\n",
      " [ 0.01435196  0.01547963 -0.00504182 -0.02935691 -0.00208431]\n",
      " [ 0.00291872 -0.00704472 -0.04038826  0.01525257 -0.03483064]\n",
      " [-0.00639649 -0.09466038 -0.07559213 -0.0681598   0.07046977]\n",
      " [ 0.05348936  0.02487477  0.07024904  0.07723635 -0.02511389]\n",
      " [ 0.06718577  0.04029544  0.02720521  0.02474062 -0.02246781]\n",
      " [-0.08267525  0.0319889   0.03807167 -0.03400251  0.08244995]\n",
      " [ 0.0649171   0.07585292  0.05543591  0.00757449 -0.06787428]\n",
      " [-0.02492717  0.06892437  0.08207031 -0.0721574   0.09007613]\n",
      " [ 0.02570036  0.08238337  0.08103435  0.03068156 -0.03278252]\n",
      " [-0.07485533  0.05879176  0.09615874 -0.01727545  0.01173386]\n",
      " [ 0.00785985  0.02833569 -0.07220946  0.00291598 -0.00085939]\n",
      " [ 0.02416662  0.0099695   0.00347817  0.03642908 -0.08789387]\n",
      " [ 0.06555734  0.04334445 -0.06891023  0.05799097 -0.03539651]\n",
      " [-0.06509853 -0.09145979 -0.09027926 -0.05491392  0.0018347 ]\n",
      " [-0.0228709  -0.10464235 -0.09699249 -0.02976553  0.04441   ]\n",
      " [-0.05772996 -0.01826551  0.09838419 -0.07783088  0.04503541]\n",
      " [ 0.09062756 -0.08728408 -0.03232266  0.04419258 -0.06696682]\n",
      " [ 0.09334444 -0.04104515 -0.08651348  0.09647688 -0.03097261]\n",
      " [-0.05357456 -0.06211337 -0.02624888 -0.00879068  0.02124463]]\n"
     ]
    }
   ],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "print(len(weights))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdcac7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05348936,  0.02487477,  0.07024904,  0.07723635, -0.02511389],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd7e7d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06718577,  0.04029544,  0.02720521,  0.02474062, -0.02246781],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9fd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next:: add pca and see how it classifies the word embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
